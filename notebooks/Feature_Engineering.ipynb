{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b127ef4f-f74c-4120-a02b-b7a796e9a2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import shap\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5295bacc-0106-4de9-b257-42d8c405b6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Colection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e9ed71-1bea-4767-ba72-cb601b4ab502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold = spark.read.table(\"projectviews.gold.projectviews_daily_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452541d2-7ff9-4cb4-b789-b3d25eca6dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba3a17e8-13e0-4b49-a528-f595de04ee15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Churn Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4d01a9-fa93-4df4-8d1b-5ec4ca698617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def detect_churn(df_gold, n=7, threshold_factor=0.3):\n",
    "    \"\"\"\n",
    "    Detect churn in a time series dataset of views per domain.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with columns ['domain_code', 'event_date', 'count_views']\n",
    "    - n: int, number of future days to consider for churn detection (default 7)\n",
    "    - threshold_factor: float, factor to multiply by past average views to define churn threshold (default 0.3)\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame with additional columns:\n",
    "        - avg_views_past_3d: rolling average views of past 3 days (excluding current day)\n",
    "        - views_plus_i: views for each of the next n days\n",
    "        - min_views_future: minimum views over the next n days\n",
    "        - threshold: threshold value for churn detection\n",
    "        - churn: 1 if minimum future views <= threshold, else 0\n",
    "    \"\"\"\n",
    "    df = df_gold.toPandas()\n",
    "\n",
    "    df = df.sort_values(['domain_code', 'event_date']).copy()\n",
    "\n",
    "    # Calculate rolling average of past 3 days (shifted by 1 day to exclude current day)\n",
    "    df['avg_views_past_3d'] = df.groupby('domain_code')['count_views']\\\n",
    "                                 .transform(lambda x: x.rolling(3, min_periods=1).mean().shift(1))\n",
    "\n",
    "    # Create columns for future views from day 1 to n\n",
    "    for i in range(1, n+1):\n",
    "        df[f'views_plus_{i}'] = df.groupby('domain_code')['count_views'].shift(-i)\n",
    "\n",
    "    # Calculate minimum views in the future window of n days\n",
    "    df['min_views_future'] = df[[f'views_plus_{i}' for i in range(1, n+1)]].min(axis=1)\n",
    "\n",
    "    # Define threshold for churn based on past average views and threshold_factor\n",
    "    df['threshold'] = df['avg_views_past_3d'] * threshold_factor\n",
    "\n",
    "    # Label churn = 1 if minimum future views <= threshold, else 0\n",
    "    df['churn'] = np.where(df['min_views_future'] <= df['threshold'], 1, 0)\n",
    "\n",
    "    # Remove rows where future data or past average is missing (usually last rows per domain)\n",
    "    df = df.dropna(subset=['min_views_future', 'avg_views_past_3d'])     \n",
    "\n",
    "    return df\n",
    "\n",
    "df_fe = detect_churn(df_gold, n=7, threshold_factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8562b61-d1bc-4123-9d17-5941b72567f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21e52046-c2c1-4dba-b035-02986ed652ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b410e7a3-dda5-4355-b0bc-92224d10c9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df_fe).createOrReplaceTempView(\"my_temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181ffd35-7da3-4c53-80d5-8ba73cef089d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT churn, count(*) FROM my_temp_view GROUP by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5832c48c-018c-4a12-857d-05e08ce0449a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754772058335}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM my_temp_view where churn =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f759eb-93cc-43e9-9f16-5841e3ed5132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dom = df_fe.copy()\n",
    "\n",
    "df_dom[\"event_date\"] = pd.to_datetime(df_dom[\"event_date\"])\n",
    "df_dom_ex = df_dom.loc[df_dom[\"domain_code\"] == 'ab.m.d']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_dom_ex[\"event_date\"], df_dom_ex[\"count_views\"], marker='o')\n",
    "plt.title(\"Daily Active Views Trend for ab.m.d\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Views\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1a63a5-2d14-43d5-9068-21a676205b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dom = df_fe.copy()\n",
    "\n",
    "df_dom[\"event_date\"] = pd.to_datetime(df_dom[\"event_date\"])\n",
    "df_dom_ex = df_dom.loc[df_dom[\"domain_code\"] == 'ace']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_dom_ex[\"event_date\"], df_dom_ex[\"count_views\"], marker='o')\n",
    "plt.title(\"Daily Active Views Trend for ace\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Views\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0246129f-75df-4346-9fc5-efb45c71295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20aaaa46-4fe4-4ba5-bac8-e30e0f09dd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 1) Churn y Retention Funnel (Overall churn)\n",
    "\n",
    "df_report = spark.createDataFrame(df_fe)\n",
    "window_spec = Window.partitionBy(\"domain_code\")\n",
    "\n",
    "df_report = df_report.select(\n",
    "    F.col(\"domain_code\"),\n",
    "    F.col(\"event_date\"),\n",
    "    F.col(\"count_views\"),\n",
    "    F.col(\"churn\")\n",
    ").withColumn(\"join_date\", F.min(\"event_date\").over(window_spec))\n",
    "\n",
    "df_report.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"projectviews.gold.vw_churn_retention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edc3927-42ee-429e-9b00-d18ece57796b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4619a776-a8f6-45ee-ae95-d14f7c6ededd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d356c3a1-7443-46d6-a314-d8451e1ba8af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_categorical_and_datetime(df, cat_columns=None, datetime_columns=None):\n",
    "    \"\"\"\n",
    "    Preprocess categorical and datetime columns.\n",
    "    \n",
    "    - Label encode categorical columns\n",
    "    - Convert datetime columns to datetime type and extract features\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original dataframe\n",
    "        cat_columns (list): List of categorical columns to encode (optional)\n",
    "        datetime_columns (list): List of datetime columns to process (optional)\n",
    "    \n",
    "    Returns:\n",
    "        df_processed (pd.DataFrame): DataFrame with processed features\n",
    "        encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    encoders = {}\n",
    "\n",
    "    # Remove nulls \n",
    "    cols_to_check = ['views_plus_2', 'views_plus_3', 'views_plus_4', 'views_plus_5', 'views_plus_6', 'views_plus_7']\n",
    "    df = df.dropna(subset=cols_to_check) \n",
    "\n",
    "    \n",
    "    # Detect categorical columns automatically if not provided\n",
    "    if cat_columns is None:\n",
    "        cat_columns = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "        # Remove datetime columns from categorical columns if datetime_columns given\n",
    "        if datetime_columns is not None:\n",
    "            cat_columns = [col for col in cat_columns if col not in datetime_columns]\n",
    "    \n",
    "    # Process datetime columns\n",
    "    if datetime_columns is None:\n",
    "        datetime_columns = []\n",
    "    \n",
    "    for col in datetime_columns:\n",
    "        df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
    "        df_processed[f'{col}_year'] = df_processed[col].dt.year\n",
    "        df_processed[f'{col}_month'] = df_processed[col].dt.month\n",
    "        df_processed[f'{col}_day'] = df_processed[col].dt.day\n",
    "        df_processed[f'{col}_dayofweek'] = df_processed[col].dt.dayofweek\n",
    "        df_processed[f'{col}_quarter'] = df_processed[col].dt.quarter\n",
    "        df_processed[f'{col}_is_weekend'] = (df_processed[col].dt.dayofweek >= 5).astype(int)\n",
    "         \n",
    "    # Process categorical columns with Label Encoding\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        encoders[col] = le\n",
    "    \n",
    "    return df_processed, encoders\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d61bc1fe-060e-4593-959b-cdb6d31ff970",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "cat_cols = ['domain_code']          \n",
    "datetime_cols = ['event_date']      \n",
    "\n",
    "df_processed, encoders = preprocess_categorical_and_datetime(df_fe, cat_columns=cat_cols, datetime_columns=datetime_cols)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d7fb709-9cd8-493b-932c-29b288ecd17a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_processed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "04059a3e-7286-43c0-99ca-893a8d277bc9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Division Train and Test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "71b9d8e4-d258-4617-b083-7e6021c08ff5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def train_test_split(df, date_column='event_date', target_column='churn', test_size=0.2):\n",
    "    \"\"\"\n",
    "    Split the dataframe into train and test sets based on date order.\n",
    "    The latest test_size portion of dates goes to test, the rest to train.\n",
    "    \"\"\"\n",
    "    df = df.sort_values(date_column)\n",
    "    \n",
    "    cutoff_idx = int(len(df) * (1 - test_size))\n",
    "    \n",
    "    train_df = df.iloc[:cutoff_idx]\n",
    "    test_df = df.iloc[cutoff_idx:]\n",
    "    \n",
    "    X_train = train_df.drop(columns=[target_column, date_column])\n",
    "    y_train = train_df[target_column]\n",
    "    \n",
    "    X_test = test_df.drop(columns=[target_column, date_column])\n",
    "    y_test = test_df[target_column]\n",
    "    \n",
    "    return X_train, X_test, y_train, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ad90faf-be5a-471f-bc94-674c3b73c63e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(df_processed, date_column='event_date', target_column='churn', test_size=0.2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03731629-9c80-4baf-9c4f-16cbd37cb5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "80f2ce5c-4bb8-4827-9255-79d4e2bf7e78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "# Crear datasets LightGBM\n",
    "train_data = lgb.Dataset(X_train, label=y_train)\n",
    "valid_data = lgb.Dataset(X_test, label=y_test)\n",
    "\n",
    "# Parámetros básicos LightGBM\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'scale_pos_weight': (len(y_train) - sum(y_train)) / sum(y_train),  # Para desbalance de clases\n",
    "    'learning_rate': 0.1,\n",
    "    'num_leaves': 31\n",
    "}\n",
    "\n",
    "# Entrenar modelo\n",
    "model = lgb.train(\n",
    "    params,\n",
    "    train_data,\n",
    "    valid_sets=[valid_data],\n",
    "    callbacks=[lgb.log_evaluation(10)]  # imprime cada 10 iteraciones\n",
    ")\n",
    "\n",
    "# Predecir en test\n",
    "y_pred_proba = model.predict(X_test)\n",
    "\n",
    "# Evaluar\n",
    "roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "pr_auc = average_precision_score(y_test, y_pred_proba)\n",
    "\n",
    "print(f\"ROC-AUC: {roc_auc:.4f}\")\n",
    "print(f\"PR-AUC: {pr_auc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "801e37c0-b393-4c9e-83d4-64842b034789",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import numpy as np\n",
    "\n",
    "def cross_validate_lgb(X, y, params, n_splits=5):\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    roc_auc_scores = []\n",
    "    pr_auc_scores = []\n",
    "    \n",
    "    for train_idx, valid_idx in cv.split(X, y):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            callbacks=[lgb.log_evaluation(10)]  \n",
    "        )\n",
    "        \n",
    "        y_pred = model.predict(X_valid)\n",
    "        roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "        pr_auc = average_precision_score(y_valid, y_pred)\n",
    "        \n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        pr_auc_scores.append(pr_auc)\n",
    "    \n",
    "    print(f\"Mean ROC-AUC: {np.mean(roc_auc_scores):.4f} ± {np.std(roc_auc_scores):.4f}\")\n",
    "    print(f\"Mean PR-AUC: {np.mean(pr_auc_scores):.4f} ± {np.std(pr_auc_scores):.4f}\")\n",
    "\n",
    "    # Calcular valores SHAP\n",
    "    explainer = shap.TreeExplainer(model)\n",
    "    shap_values = explainer.shap_values(X)\n",
    "\n",
    "    # Gráfico resumen de importancia SHAP\n",
    "    shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "\n",
    "    # Gráfico de dispersión SHAP para las variables top 5\n",
    "    shap.summary_plot(shap_values, X)\n",
    "\n",
    "# Ejemplo de parámetros básicos\n",
    "params = {\n",
    "    'objective': 'binary',\n",
    "    'metric': 'auc',\n",
    "    'boosting_type': 'gbdt',\n",
    "    'learning_rate': 0.05,\n",
    "    'num_leaves': 31,\n",
    "    'verbosity': -1,\n",
    "    'seed': 42,\n",
    "    'scale_pos_weight': (len(y) - sum(y)) / sum(y)  # balance class imbalance\n",
    "}\n",
    "\n",
    "\n",
    "X = df_processed.drop(columns=['churn', 'event_date'])\n",
    "y = df_processed['churn']\n",
    "cross_validate_lgb(X, y, params)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a7fd57dc-8783-4c6d-964c-cf0a9eb461b1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7869073159749080,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

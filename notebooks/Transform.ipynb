{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8d4689e0-4100-4b9e-b0f8-a21ea0803d43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession, DataFrame\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StringType, LongType, TimestampType, DateType\n",
    ")\n",
    "from datetime import datetime\n",
    "import json\n",
    "import uuid\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# ---------------------------\n",
    "# Config\n",
    "# ---------------------------\n",
    "VOLUME_PATH      = \"/Volumes/projectviews/bronze/raw_files\"   # bronze volume path\n",
    "SILVER_TABLE     = \"projectviews.silver.projectviews_clean\"\n",
    "QUAR_TABLE       = \"projectviews.silver.projectviews_quarantine\"\n",
    "DQ_REPORT_TABLE  = \"projectviews.silver.dq_reports\"\n",
    "\n",
    "DQ_CONFIG = {\n",
    "    \"domain_code_regex\": r\"^[a-z]+(\\.[a-z]+)*$\",\n",
    "    \"max_null_pct\": 0.05,                # if > 5% null in critical col -> alert (kept for report)\n",
    "    \"min_rows_expected\": 1\n",
    "}\n",
    "\n",
    "RUN_ID = str(uuid.uuid4())  # trace a single run across outputs\n",
    "\n",
    "# ---------------------------\n",
    "# 1) Parse raw DataFrame\n",
    "# ---------------------------\n",
    "def parse_raw_df(bronze_path: str) -> DataFrame:\n",
    "    \"\"\"\n",
    "    Read raw plain text lines from bronze volume and extract:\n",
    "      - domain_code (field 0)\n",
    "      - count_views (field 2)\n",
    "      - event_timestamp/event_date from filename: projectviews-YYYYMMDD-HHMMSS.*\n",
    "    \"\"\"\n",
    "    raw = (\n",
    "        spark.read.format(\"text\")\n",
    "        .option(\"recursiveFileLookup\", \"true\")\n",
    "        .load(bronze_path)\n",
    "        .filter(~F.col(\"value\").startswith('\"\"'))  # keep as-is from your snippet\n",
    "    )\n",
    "\n",
    "    # Split line into 4 tokens: we keep 0 and 2 as in your code\n",
    "    split_col = F.split(F.col(\"value\"), \" \", 4)\n",
    "\n",
    "    parsed = (\n",
    "        raw\n",
    "        .withColumn(\"split\", split_col)\n",
    "        .filter(F.size(\"split\") == 4)\n",
    "        .select(\n",
    "            F.col(\"split\")[0].alias(\"domain_code\"),\n",
    "            F.col(\"split\")[2].cast(\"long\").alias(\"count_views\"),\n",
    "            F.col(\"_metadata.file_path\").alias(\"file_path\")\n",
    "        )\n",
    "        .withColumn(\"date_str\", F.regexp_extract(F.col(\"file_path\"), r\"projectviews-(\\d{8})-(\\d{6})\", 1))\n",
    "        .withColumn(\"time_str\", F.regexp_extract(F.col(\"file_path\"), r\"projectviews-(\\d{8})-(\\d{6})\", 2))\n",
    "        .withColumn(\n",
    "            \"datetime_concat\",\n",
    "            F.when(\n",
    "                (F.col(\"date_str\") != \"\") & (F.col(\"time_str\") != \"\"),\n",
    "                F.concat_ws(\"\", F.col(\"date_str\"), F.col(\"time_str\"))\n",
    "            )\n",
    "        )\n",
    "        .withColumn(\n",
    "            \"event_timestamp\",\n",
    "            F.when(\n",
    "                F.length(F.col(\"datetime_concat\")) == 14,\n",
    "                F.to_timestamp(F.col(\"datetime_concat\"), \"yyyyMMddHHmmss\")\n",
    "            ).cast(TimestampType())\n",
    "        )\n",
    "        .withColumn(\"event_date\", F.to_date(F.col(\"event_timestamp\")).cast(DateType()))\n",
    "        .withColumn(\"run_id\", F.lit(RUN_ID))\n",
    "        .drop(\"date_str\", \"time_str\", \"datetime_concat\")\n",
    "    )\n",
    "\n",
    "    # Optional: repartition by event_date for better downstream IO\n",
    "    parsed = parsed.repartition(\"event_date\")\n",
    "    return parsed\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Enforce schema + casts\n",
    "# ---------------------------\n",
    "def enforce_and_cast_schema(df: DataFrame) -> DataFrame:\n",
    "    # make sure all expected columns exist and have consistent types\n",
    "    needed = {\n",
    "        \"domain_code\": StringType(),\n",
    "        \"count_views\": LongType(),\n",
    "        \"event_timestamp\": TimestampType(),\n",
    "        \"event_date\": DateType(),\n",
    "        \"run_id\": StringType(),\n",
    "        \"file_path\": StringType()\n",
    "    }\n",
    "    for c, t in needed.items():\n",
    "        if c not in df.columns:\n",
    "            df = df.withColumn(c, F.lit(None).cast(t))\n",
    "        else:\n",
    "            df = df.withColumn(c, F.col(c).cast(t))\n",
    "    return df\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 3) DQ checks\n",
    "# ---------------------------\n",
    "def run_dq_checks(df: DataFrame, config: dict = DQ_CONFIG) -> dict:\n",
    "    \"\"\"\n",
    "    Returns dict with:\n",
    "      - valid_df: rows passing checks\n",
    "      - quarantine_df: rows failing >=1 check\n",
    "      - dq_report: small DF with JSON report\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "\n",
    "    expected_cols = [\"domain_code\", \"count_views\", \"event_timestamp\", \"event_date\"]\n",
    "    missing_cols = [c for c in expected_cols if c not in df.columns]\n",
    "\n",
    "    # Null counts/pct (single pass per column)\n",
    "    null_counts = {}\n",
    "    null_pct = {}\n",
    "    for c in expected_cols:\n",
    "        cnt = df.filter(F.col(c).isNull() | (F.trim(F.col(c).cast(\"string\")) == \"\")).count() if c in df.columns else total_rows\n",
    "        null_counts[c] = cnt\n",
    "        null_pct[c] = (cnt / total_rows) if total_rows > 0 else None\n",
    "\n",
    "    # Domain regex validity\n",
    "    regex = config[\"domain_code_regex\"]\n",
    "    invalid_domain_df = df.filter(F.col(\"domain_code\").isNull() | (~F.col(\"domain_code\").rlike(regex)))\n",
    "    invalid_domain_cnt = invalid_domain_df.count()\n",
    "\n",
    "    # Non-negative\n",
    "    negative_vals_df = df.filter(F.col(\"count_views\") < 0)\n",
    "    negative_cnt = negative_vals_df.count()\n",
    "\n",
    "    # Build quarantine (union)\n",
    "    failing_frames = []\n",
    "    if invalid_domain_cnt > 0:\n",
    "        failing_frames.append(invalid_domain_df.withColumn(\"dq_reason\", F.lit(\"invalid_domain\")))\n",
    "    if negative_cnt > 0:\n",
    "        failing_frames.append(negative_vals_df.withColumn(\"dq_reason\", F.lit(\"negative_values\")))\n",
    "\n",
    "    if failing_frames:\n",
    "        quarantine_df = failing_frames[0]\n",
    "        for f in failing_frames[1:]:\n",
    "            quarantine_df = quarantine_df.unionByName(f, allowMissingColumns=True)\n",
    "        quarantine_df = quarantine_df.dropDuplicates()\n",
    "        # valid = df EXCEPT quarantine (full-row hash anti-join)\n",
    "        from pyspark.sql.functions import sha2, concat_ws\n",
    "        hash_col = \"row_hash\"\n",
    "        df_h = df.withColumn(hash_col, sha2(concat_ws(\"||\", *df.columns), 256))\n",
    "        q_h  = quarantine_df.withColumn(hash_col, sha2(concat_ws(\"||\", *quarantine_df.columns), 256))\n",
    "        valid_df = df_h.join(q_h.select(hash_col).withColumn(\"flag\", F.lit(1)), on=hash_col, how=\"left_anti\").drop(hash_col)\n",
    "    else:\n",
    "        quarantine_df = df.limit(0)\n",
    "        valid_df = df\n",
    "\n",
    "    dq_summary = {\n",
    "        \"run_id\": RUN_ID,\n",
    "        \"total_rows\": total_rows,\n",
    "        \"checks\": {\n",
    "            \"missing_columns\": missing_cols,\n",
    "            \"null_counts\": null_counts,\n",
    "            \"null_pct\": null_pct,\n",
    "            \"invalid_domain_count\": invalid_domain_cnt,\n",
    "            \"negative_values_count\": negative_cnt,\n",
    "            \"min_rows_ok\": total_rows >= config.get(\"min_rows_expected\", 1),\n",
    "        },\n",
    "        \"generated_at\": datetime.utcnow().isoformat() + \"Z\"\n",
    "    }\n",
    "\n",
    "    dq_report_df = spark.createDataFrame(\n",
    "        [(json.dumps(dq_summary), RUN_ID, datetime.utcnow())],\n",
    "        [\"dq_report_json\", \"run_id\", \"execution_timestamp\"]\n",
    "    )\n",
    "\n",
    "    return {\"valid_df\": valid_df, \"quarantine_df\": quarantine_df, \"dq_report\": dq_report_df}\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Write outputs (Delta UC tables)\n",
    "# ---------------------------\n",
    "def write_outputs(valid_df: DataFrame, quarantine_df: DataFrame, dq_report_df: DataFrame,\n",
    "                  silver_table: str, quarantine_table: str, dq_report_table: str,\n",
    "                  partition_by: list = [\"event_date\"]):\n",
    "    (\n",
    "        valid_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\") # append\n",
    "        .partitionBy(*partition_by)\n",
    "        .saveAsTable(silver_table)\n",
    "    )\n",
    "\n",
    "    if quarantine_df.count() > 0:\n",
    "        (\n",
    "            quarantine_df.write\n",
    "            .format(\"delta\")\n",
    "            .mode(\"overwrite\") # append\n",
    "            .saveAsTable(quarantine_table)\n",
    "        )\n",
    "\n",
    "    (\n",
    "        dq_report_df.write\n",
    "        .format(\"delta\")\n",
    "        .mode(\"overwrite\") # append\n",
    "        .saveAsTable(dq_report_table)\n",
    "    )\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# Orchestration (run)\n",
    "# ---------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    parsed = parse_raw_df(VOLUME_PATH)\n",
    "    parsed_c = enforce_and_cast_schema(parsed)\n",
    "    results = run_dq_checks(parsed_c, config=DQ_CONFIG)\n",
    "\n",
    "    write_outputs(\n",
    "        results[\"valid_df\"],\n",
    "        results[\"quarantine_df\"],\n",
    "        results[\"dq_report\"],\n",
    "        SILVER_TABLE,\n",
    "        QUAR_TABLE,\n",
    "        DQ_REPORT_TABLE\n",
    "    )\n",
    "\n",
    "    print(f\"Silver written: {SILVER_TABLE} | Quarantine: {QUAR_TABLE} | DQ: {DQ_REPORT_TABLE} | run_id={RUN_ID}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "7829152c-8dba-4222-9151-f1ad9e9e56aa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5151132793875100,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

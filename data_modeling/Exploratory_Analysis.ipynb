{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26a84945-c00c-4808-82db-42a67f613535",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d5dd5e9-abb3-4a6f-9cf3-9ac60f3c17e9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "\n",
    "def plot_engagement_kpis_with_quirks(df):\n",
    "    \"\"\"\n",
    "    Calculate and plot DAU, WAU, MAU trends from a PySpark DataFrame,\n",
    "    and highlight possible data quirks (seasonality, sparsity, spikes).\n",
    "    \n",
    "    DataFrame must have:\n",
    "        - event_date (date or string)\n",
    "        - count_views (numeric)\n",
    "    \"\"\"\n",
    "\n",
    "    df = df.withColumn(\"event_date\", F.to_date(\"event_date\"))\n",
    "\n",
    "    dau_df = (df.groupBy(\"event_date\")\n",
    "                .agg(F.sum(\"count_views\").alias(\"DAU_views\"))\n",
    "                .orderBy(\"event_date\"))\n",
    "\n",
    "    df_week = df.withColumn(\"week\", F.date_trunc(\"week\", \"event_date\"))\n",
    "    wau_df = (df_week.groupBy(\"week\")\n",
    "                .agg(F.sum(\"count_views\").alias(\"WAU_views\"))\n",
    "                .orderBy(\"week\"))\n",
    "\n",
    "    df_month = df.withColumn(\"month\", F.date_trunc(\"month\", \"event_date\"))\n",
    "    mau_df = (df_month.groupBy(\"month\")\n",
    "                .agg(F.sum(\"count_views\").alias(\"MAU_views\"))\n",
    "                .orderBy(\"month\"))\n",
    "\n",
    "    dau_pd = dau_df.toPandas()\n",
    "    wau_pd = wau_df.toPandas()\n",
    "    mau_pd = mau_df.toPandas()\n",
    "\n",
    "\n",
    "    dau_pd[\"event_date\"] = pd.to_datetime(dau_pd[\"event_date\"])\n",
    "    wau_pd[\"week\"] = pd.to_datetime(wau_pd[\"week\"])\n",
    "    mau_pd[\"month\"] = pd.to_datetime(mau_pd[\"month\"])\n",
    "\n",
    "\n",
    "    plt.figure(figsize=(10,5))\n",
    "    plt.plot(dau_pd[\"event_date\"], dau_pd[\"DAU_views\"], marker='o', label=\"DAU\")\n",
    "    plt.title(\"Daily Active Views Trend\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Views\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    # --- Detect quirks ---\n",
    "    quirks = {}\n",
    "\n",
    "    dau_pd['event_date'] = pd.to_datetime(dau_pd['event_date'])\n",
    "\n",
    "    ###################################\n",
    "    # 1. Sparsity: days with zero views\n",
    "    ###################################\n",
    "\n",
    "    # Define the full date range from the min to the max date in your data\n",
    "    min_date = dau_pd['event_date'].min()\n",
    "    max_date = dau_pd['event_date'].max()\n",
    "    full_dates = pd.date_range(start=min_date, end=max_date)\n",
    "\n",
    "    # Days with zero views - guardamos el DataFrame para plot\n",
    "    zero_days_df = dau_pd[dau_pd[\"DAU_views\"] == 0]\n",
    "\n",
    "    # Days missing from the dataset\n",
    "    missing_days = full_dates.difference(dau_pd['event_date'])\n",
    "\n",
    "    quirks[\"sparsity_zero_views\"] = zero_days_df[\"event_date\"].tolist()\n",
    "    quirks[\"sparsity_missing_days\"] = missing_days.tolist()\n",
    "\n",
    "    print(\"Days with zero views:\", quirks[\"sparsity_zero_views\"])\n",
    "    print(\"Days missing from dataset:\", quirks[\"sparsity_missing_days\"])\n",
    "\n",
    "    #############################################\n",
    "    # 2. Spikes: values greater than mean + 3*std\n",
    "    #############################################\n",
    "\n",
    "    threshold = dau_pd[\"DAU_views\"].mean() + 3 * dau_pd[\"DAU_views\"].std()\n",
    "    spikes_df = dau_pd[dau_pd[\"DAU_views\"] > threshold]\n",
    "    quirks[\"spikes\"] = spikes_df[\"event_date\"].tolist()\n",
    "\n",
    "    #####################\n",
    "    # 3. Seasonality hint\n",
    "    #####################\n",
    "\n",
    "    weekly_avg = dau_pd.groupby(dau_pd[\"event_date\"].dt.dayofweek)[\"DAU_views\"].mean()\n",
    "    quirks[\"seasonality\"] = weekly_avg.to_dict()\n",
    "\n",
    "    # --- Plot ---\n",
    "    plt.figure(figsize=(14,6))\n",
    "    plt.plot(dau_pd[\"event_date\"], dau_pd[\"DAU_views\"], marker='o', label=\"DAU\")\n",
    "    plt.plot(wau_pd[\"week\"], wau_pd[\"WAU_views\"], marker='s', label=\"WAU\")\n",
    "    plt.plot(mau_pd[\"month\"], mau_pd[\"MAU_views\"], marker='^', label=\"MAU\")\n",
    "\n",
    "    # Mark zero days\n",
    "    plt.scatter(zero_days_df[\"event_date\"], zero_days_df[\"DAU_views\"], color='red', label=\"Zero days\", zorder=5)\n",
    "\n",
    "    # Mark spikes\n",
    "    plt.scatter(spikes_df[\"event_date\"], spikes_df[\"DAU_views\"], color='orange', label=\"Spikes\", zorder=5)\n",
    "\n",
    "    plt.title(\"Engagement KPIs with Data Quirks Highlighted\")\n",
    "    plt.xlabel(\"Date\")\n",
    "    plt.ylabel(\"Views\")\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "    # --- Print quirks summary ---\n",
    "    print(\"\\n=== Data Quirks Detected ===\")\n",
    "    print(f\"1. Sparsity (zero days): {quirks['sparsity_zero_views']}\")\n",
    "    print(f\"   Missing days: {quirks['sparsity_missing_days']}\")\n",
    "    print(f\"2. Spikes: {quirks['spikes']}\")\n",
    "    print(\"3. Seasonality pattern (avg views by weekday):\")\n",
    "    for day, avg in quirks[\"seasonality\"].items():\n",
    "        print(f\"   Day {day} (0=Mon): {avg:.2f}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc4a4d1b-4716-47af-a422-327d18c0c301",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"projectviews.gold.projectviews_daily_summary\")\n",
    "plot_engagement_kpis_with_quirks(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "efe82e6c-182e-4399-8143-70b2013fad38",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def plot_avg_views_by_weekday(df, date_col=\"event_date\", views_col=\"count_views\"):\n",
    "    \"\"\"\n",
    "    Calculate average views by weekday and plot the result.\n",
    "\n",
    "    Parameters:\n",
    "    - df: Spark DataFrame with at least two columns: a date column and a views count column.\n",
    "    - date_col: Name of the date column in df (default \"event_date\").\n",
    "    - views_col: Name of the views count column in df (default \"count_views\").\n",
    "    \"\"\"\n",
    "\n",
    "    # Add a weekday number column (Monday=0, Sunday=6)\n",
    "    df_with_weekday = df.withColumn(\n",
    "        \"weekday\",\n",
    "        ((F.dayofweek(F.col(date_col)) + 5) % 7)\n",
    "    ).withColumn(\n",
    "        \"weekday_name\",\n",
    "        F.when(F.col(\"weekday\") == 0, \"Monday\")\n",
    "         .when(F.col(\"weekday\") == 1, \"Tuesday\")\n",
    "         .when(F.col(\"weekday\") == 2, \"Wednesday\")\n",
    "         .when(F.col(\"weekday\") == 3, \"Thursday\")\n",
    "         .when(F.col(\"weekday\") == 4, \"Friday\")\n",
    "         .when(F.col(\"weekday\") == 5, \"Saturday\")\n",
    "         .when(F.col(\"weekday\") == 6, \"Sunday\")\n",
    "    )\n",
    "\n",
    "    # Aggregate average views by weekday name\n",
    "    agg_df = df_with_weekday.groupBy(\"weekday_name\") \\\n",
    "        .agg(F.avg(views_col).alias(\"avg_views\"))\n",
    "\n",
    "    # Collect to pandas\n",
    "    pdf = agg_df.toPandas()\n",
    "\n",
    "    # Order the days of the week properly\n",
    "    order = [\"Monday\", \"Tuesday\", \"Wednesday\", \"Thursday\", \"Friday\", \"Saturday\", \"Sunday\"]\n",
    "    pdf['weekday_name'] = pd.Categorical(pdf['weekday_name'], categories=order, ordered=True)\n",
    "    pdf = pdf.sort_values('weekday_name')\n",
    "\n",
    "    # Plotting\n",
    "    plt.figure(figsize=(10,6))\n",
    "    plt.bar(pdf['weekday_name'], pdf['avg_views'], color='skyblue')\n",
    "    plt.title('Average Views by Weekday')\n",
    "    plt.xlabel('Weekday')\n",
    "    plt.ylabel('Average Views')\n",
    "    plt.grid(axis='y', linestyle='--', alpha=0.7)\n",
    "    plt.show()\n",
    "\n",
    "plot_avg_views_by_weekday(df)\n"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "03a4cfc5-7a0e-4d9a-b8e9-95ef16ece479",
     "origId": 7869073159749023,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 7869073159749014,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Exploratory_Analysis",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fa159b-8a33-4eb7-a59a-1b6afd179d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window, DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, LongType, TimestampType, DateType\n",
    "import json\n",
    "import time\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3a725be-7fd4-4118-8bb1-2e84903f4cd7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "DQ_CONFIG = {\n",
    "    \"domain_code_regex\": r\"^[a-z]+(\\.[a-z]+)*$\",\n",
    "    \"max_null_pct\": 0.05,                # si > 5% nulos en columna crit -> alerta\n",
    "    \"min_rows_expected\": 1\n",
    "}\n",
    "\n",
    "EXPECTED_SCHEMA = StructType([\n",
    "    StructField(\"domain_code\", StringType(), True),\n",
    "    StructField(\"count_views\", IntegerType(), True),\n",
    "    StructField(\"total_response_size\", LongType(), True),\n",
    "    StructField(\"event_timestamp\", TimestampType(), True),\n",
    "    StructField(\"event_date\", DateType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "86b527aa-a184-4112-bfa2-13148a006b8c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# ---------------------------\n",
    "# 1) Parse raw DataFrame \n",
    "# ---------------------------\n",
    "\n",
    "def parse_raw_df(bronze_table) -> DataFrame:\n",
    "    parsed = (\n",
    "        spark\n",
    "        .read\n",
    "        .text(bronze_path)\n",
    "        .filter(~F.col(\"value\").startswith('\"\"'))   \n",
    "        .withColumn(\"split\", F.split(F.col(\"value\"), \" \", 4))\n",
    "        .filter(F.size(\"split\") == 4)\n",
    "        .select(\n",
    "            F.col(\"split\")[0].alias(\"domain_code\"),\n",
    "            F.col(\"split\")[2].cast(\"int\").alias(\"count_views\"),\n",
    "            F.col(\"split\")[3].cast(\"long\").alias(\"total_response_size\"),\n",
    "            F.col(\"_metadata.file_path\").alias(\"file_path\")\n",
    "        )\n",
    "        .withColumn(\"date_str\", F.regexp_extract(F.col(\"file_path\"), r\"projectviews-(\\d{8})-(\\d{6})\", 1))\n",
    "        .withColumn(\"time_str\", F.regexp_extract(F.col(\"file_path\"), r\"projectviews-(\\d{8})-(\\d{6})\", 2))\n",
    "        .withColumn(\"datetime_concat\", \n",
    "                   F.when((F.col(\"date_str\") != \"\") & (F.col(\"time_str\") != \"\"), \n",
    "                         F.concat_ws(\"\", F.col(\"date_str\"), F.col(\"time_str\")))\n",
    "                   .otherwise(None))\n",
    "        .withColumn(\"event_timestamp\", \n",
    "                   F.when(F.col(\"datetime_concat\").isNotNull() & (F.length(F.col(\"datetime_concat\")) == 14),\n",
    "                         F.to_timestamp(F.col(\"datetime_concat\"), \"yyyyMMddHHmmss\"))\n",
    "                   .otherwise(None))\n",
    "        .withColumn(\"event_date\", F.to_date(F.col(\"event_timestamp\")))\n",
    "        .drop(\"date_str\", \"time_str\", \"datetime_concat\", \"file_path\")\n",
    "    )\n",
    "\n",
    "    parsed = parsed.repartition(\"event_date\")\n",
    "\n",
    "    return parsed\n",
    "\n",
    "# ---------------------------\n",
    "# 2) Enforce schema + casts\n",
    "# ---------------------------\n",
    "\n",
    "def enforce_and_cast_schema(df: DataFrame) -> DataFrame:\n",
    "    cols = df.columns\n",
    "    for c in [\"domain_code\", \"count_views\", \"total_response_size\", \"event_timestamp\", \"event_date\"]:\n",
    "        if c not in cols:\n",
    "            df = df.withColumn(c, F.lit(None).cast(StringType() if c==\"domain_code\" else IntegerType()))\n",
    "    df = (\n",
    "        df\n",
    "        .withColumn(\"domain_code\", F.col(\"domain_code\").cast(\"string\"))\n",
    "        .withColumn(\"count_views\", F.col(\"count_views\").cast(\"long\"))  \n",
    "        .withColumn(\"total_response_size\", F.col(\"total_response_size\").cast(\"long\"))\n",
    "        .withColumn(\"event_timestamp\", F.col(\"event_timestamp\").cast(\"timestamp\"))\n",
    "        .withColumn(\"event_date\", F.to_date(F.col(\"event_date\")))\n",
    "    )\n",
    "    return df\n",
    "\n",
    "# ---------------------------\n",
    "# 3) DQ checks\n",
    "# ---------------------------\n",
    "\n",
    "def run_dq_checks(df: DataFrame, config: dict = DQ_CONFIG, spark=None, quarantine_path: str = None) -> dict:\n",
    "    \"\"\"\n",
    "    Ejecuta una serie de checks y retorna:\n",
    "      - valid_df: filas que pasan todos los checks\n",
    "      - quarantine_df: filas que fallan al menos un check (para inspección)\n",
    "      - dq_report: dict con métricas y resultados\n",
    "    \"\"\"\n",
    "    total_rows = df.count()\n",
    "    dq_report = {\"total_rows\": total_rows, \"checks\": {}}\n",
    "\n",
    "    # 3.1 Schema presence (columns)\n",
    "    expected_cols = [\"domain_code\", \"count_views\", \"total_response_size\", \"event_timestamp\", \"event_date\"]\n",
    "    missing_cols = [c for c in expected_cols if c not in df.columns]\n",
    "    dq_report[\"checks\"][\"missing_columns\"] = missing_cols\n",
    "\n",
    "    # 3.2 Nulls and completeness con cast seguro a string\n",
    "    null_counts = {}\n",
    "    null_pct = {}\n",
    "    for c in [\"domain_code\", \"count_views\", \"total_response_size\", \"event_timestamp\", \"event_date\"]:\n",
    "        # FIX: Verificar que la columna existe antes de hacer el check\n",
    "        if c in df.columns:\n",
    "            cnt = df.filter(\n",
    "                F.col(c).isNull() | (F.trim(F.col(c).cast(\"string\")) == \"\")\n",
    "            ).count()\n",
    "            null_counts[c] = cnt\n",
    "            null_pct[c] = cnt / total_rows if total_rows > 0 else None\n",
    "        else:\n",
    "            null_counts[c] = total_rows  # Si la columna no existe, todas las filas son \"null\"\n",
    "            null_pct[c] = 1.0\n",
    "    dq_report[\"checks\"][\"null_counts\"] = null_counts\n",
    "    dq_report[\"checks\"][\"null_pct\"] = null_pct\n",
    "\n",
    "    # 3.3 Domain regex validity\n",
    "    regex = config[\"domain_code_regex\"]\n",
    "    invalid_domain_df = df.filter((F.col(\"domain_code\").isNull()) | (~F.col(\"domain_code\").rlike(regex)))\n",
    "    invalid_domain_cnt = invalid_domain_df.count()\n",
    "    dq_report[\"checks\"][\"invalid_domain_count\"] = invalid_domain_cnt\n",
    "\n",
    "    # 3.4 Non-negative ranges\n",
    "    negative_vals_df = df.filter((F.col(\"count_views\") < 0) | (F.col(\"total_response_size\") < 0))\n",
    "    negative_cnt = negative_vals_df.count()\n",
    "    dq_report[\"checks\"][\"negative_values_count\"] = negative_cnt\n",
    "\n",
    "    # 3.10 Minimal row count\n",
    "    dq_report[\"checks\"][\"min_rows_ok\"] = total_rows >= config.get(\"min_rows_expected\", 1)\n",
    "\n",
    "    # Construir quarantine_df (filas que fallan algún check)\n",
    "    failing_frames = []\n",
    "    if invalid_domain_cnt > 0:\n",
    "        failing_frames.append(invalid_domain_df.withColumn(\"dq_reason\", F.lit(\"invalid_domain\")))\n",
    "    if negative_cnt > 0:\n",
    "        failing_frames.append(negative_vals_df.withColumn(\"dq_reason\", F.lit(\"negative_values\")))\n",
    "    if failing_frames:\n",
    "        quarantine_df = failing_frames[0]\n",
    "        for f in failing_frames[1:]:\n",
    "            quarantine_df = quarantine_df.unionByName(f, allowMissingColumns=True)\n",
    "        quarantine_df = quarantine_df.dropDuplicates()  # Sin ingest_id, deduplico todo el row\n",
    "    else:\n",
    "        quarantine_df = df.limit(0)  # vacío\n",
    "\n",
    "    # valid_df = df - quarantine_df por left_anti join usando todas las columnas\n",
    "    if quarantine_df.count() > 0:\n",
    "        # Crear columna de hash para comparar filas completas\n",
    "        from pyspark.sql.functions import sha2, concat_ws\n",
    "\n",
    "        hash_col = \"row_hash\"\n",
    "        df_with_hash = df.withColumn(hash_col, sha2(concat_ws(\"||\", *df.columns), 256))\n",
    "        quarantine_with_hash = quarantine_df.withColumn(hash_col, sha2(concat_ws(\"||\", *quarantine_df.columns), 256))\n",
    "\n",
    "        valid_df = df_with_hash.join(quarantine_with_hash.select(hash_col).withColumn(\"flag\", F.lit(1)),\n",
    "                                    on=hash_col, how=\"left_anti\").drop(hash_col)\n",
    "    else:\n",
    "        valid_df = df\n",
    "\n",
    "    dq_report[\"counts\"] = {\n",
    "        \"total_rows\": total_rows,\n",
    "        \"valid_rows\": valid_df.count(),\n",
    "        \"quarantine_rows\": quarantine_df.count()\n",
    "    }\n",
    "\n",
    "    dq_report_df = spark.createDataFrame(\n",
    "        [(json.dumps(dq_report), datetime.now())],\n",
    "        [\"dq_report_json\", \"execution_timestamp\"]\n",
    "    )\n",
    "\n",
    "    return {\n",
    "        \"valid_df\": valid_df,\n",
    "        \"quarantine_df\": quarantine_df,\n",
    "        \"dq_report\": dq_report_df\n",
    "    }\n",
    "\n",
    "\n",
    "# ---------------------------\n",
    "# 4) Write outputs (Delta or Parquet) + save DQ report\n",
    "# ---------------------------\n",
    "def write_outputs(valid_df: DataFrame, quarantine_df: DataFrame, dq_report_df: DataFrame,\n",
    "                  silver_path: str, quarantine_path: str, dq_report_path: str,\n",
    "                  partition_by: list = [\"event_date\"]):\n",
    "    \"\"\"\n",
    "    Escribe valid_df a silver (delta), quarantine rows a quarantine_path (parquet),\n",
    "    y dq_report (json) a dq_report_path.\n",
    "    \"\"\"\n",
    "    # write silver as Delta (append)\n",
    "    (valid_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "     .mode(\"overwrite\")\n",
    "     .partitionBy(*partition_by)\n",
    "     .saveAsTable(silver_path)\n",
    "    )\n",
    "\n",
    "    # write quarantine as parquet (append)\n",
    "    if quarantine_df.count() > 0:\n",
    "        (quarantine_df\n",
    "         .write\n",
    "         .option(\"mergeSchema\", \"true\")\n",
    "         .format(\"delta\")\n",
    "         .mode(\"overwrite\")\n",
    "         .saveAsTable(quarantine_path)\n",
    "        )\n",
    "\n",
    "    (dq_report_df\n",
    "     .write\n",
    "     .format(\"delta\")\n",
    "     .option(\"mergeSchema\", \"true\")\n",
    "    .mode(\"overwrite\")\n",
    "     .saveAsTable(dq_report_path)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bf8b3c66-3e70-44f0-966e-71f6dae8faf2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "bronze_path = \"/Volumes/projectviews/bronze/raw_files\"\n",
    "silver_table = \"projectviews.silver.projectviews_clean\"\n",
    "quarantine_path = \"projectviews.silver.projectviews_quarantine\"\n",
    "dq_report_path = \"projectviews.silver.dq_reports\"\n",
    "\n",
    "parsed = parse_raw_df(bronze_path)\n",
    "parsed_check = enforce_and_cast_schema(parsed)\n",
    "results = run_dq_checks(parsed_check, config=DQ_CONFIG, spark=spark)\n",
    "\n",
    "valid_df = results[\"valid_df\"]\n",
    "quarantine_df = results[\"quarantine_df\"]\n",
    "dq_report_df = results[\"dq_report\"]\n",
    "write_outputs(valid_df, quarantine_df, dq_report_df, silver_table, quarantine_path, dq_report_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "050949dc-edce-477b-b5b5-5c3ab23152d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql \n",
    "with cte as (\n",
    "  select domain_code, event_date,\n",
    "sum(count_views) as count_views,\n",
    "sum(total_response_size) as total_response_size \n",
    "from projectviews.silver.projectviews_clean \n",
    "group by 1, 2\n",
    ")\n",
    "\n",
    "select count(*) from cte"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5151132793875100,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Transform",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b127ef4f-f74c-4120-a02b-b7a796e9a2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "import optuna\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "import shap\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, roc_curve, auc, precision_recall_curve, average_precision_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5295bacc-0106-4de9-b257-42d8c405b6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Data Collection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e9ed71-1bea-4767-ba72-cb601b4ab502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold = spark.read.table(\"projectviews.gold.projectviews_daily_summary\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "452541d2-7ff9-4cb4-b789-b3d25eca6dc7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_gold.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba3a17e8-13e0-4b49-a528-f595de04ee15",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Define Churn Conditions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1a4d01a9-fa93-4df4-8d1b-5ec4ca698617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def detect_churn(df_gold, n=7, threshold_factor=0.3):\n",
    "    \"\"\"\n",
    "    Detect churn in a time series dataset of views per domain.\n",
    "\n",
    "    Parameters:\n",
    "    - df: pandas DataFrame with columns ['domain_code', 'event_date', 'count_views']\n",
    "    - n: int, number of future days to consider for churn detection (default 7)\n",
    "    - threshold_factor: float, factor to multiply by past average views to define churn threshold (default 0.3)\n",
    "\n",
    "    Returns:\n",
    "    - df: pandas DataFrame with additional columns:\n",
    "        - avg_views_past_3d: rolling average views of past 3 days (excluding current day)\n",
    "        - views_plus_i: views for each of the next n days\n",
    "        - min_views_future: minimum views over the next n days\n",
    "        - threshold: threshold value for churn detection\n",
    "        - churn: 1 if minimum future views <= threshold, else 0\n",
    "    \"\"\"\n",
    "    df = df_gold.toPandas()\n",
    "\n",
    "    df = df.sort_values(['domain_code', 'event_date']).copy()\n",
    "\n",
    "    # Calculate rolling average of past 3 days (shifted by 1 day to exclude current day)\n",
    "    df['avg_views_past_3d'] = df.groupby('domain_code')['count_views']\\\n",
    "                                 .transform(lambda x: x.rolling(3, min_periods=1).mean().shift(1))\n",
    "\n",
    "    # Create columns for future views from day 1 to n\n",
    "    for i in range(1, n+1):\n",
    "        df[f'views_plus_{i}'] = df.groupby('domain_code')['count_views'].shift(-i)\n",
    "\n",
    "    # Calculate minimum views in the future window of n days\n",
    "    df['min_views_future'] = df[[f'views_plus_{i}' for i in range(1, n+1)]].min(axis=1)\n",
    "\n",
    "    # Define threshold for churn based on past average views and threshold_factor\n",
    "    df['threshold'] = df['avg_views_past_3d'] * threshold_factor\n",
    "\n",
    "    # Label churn = 1 if minimum future views <= threshold, else 0\n",
    "    df['churn'] = np.where(df['min_views_future'] <= df['threshold'], 1, 0)\n",
    "\n",
    "    # Remove rows where future data or past average is missing (usually last rows per domain)\n",
    "    df = df.dropna(subset=['min_views_future', 'avg_views_past_3d'])     \n",
    "\n",
    "    return df\n",
    "\n",
    "df_fe = detect_churn(df_gold, n=7, threshold_factor=0.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e8562b61-d1bc-4123-9d17-5941b72567f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_fe.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "21e52046-c2c1-4dba-b035-02986ed652ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b410e7a3-dda5-4355-b0bc-92224d10c9f8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "spark.createDataFrame(df_fe).createOrReplaceTempView(\"my_temp_view\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "181ffd35-7da3-4c53-80d5-8ba73cef089d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT churn, count(*) FROM my_temp_view GROUP by 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5832c48c-018c-4a12-857d-05e08ce0449a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1754772058335}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM my_temp_view where churn =1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b8f759eb-93cc-43e9-9f16-5841e3ed5132",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dom = df_fe.copy()\n",
    "\n",
    "df_dom[\"event_date\"] = pd.to_datetime(df_dom[\"event_date\"])\n",
    "df_dom_ex = df_dom.loc[df_dom[\"domain_code\"] == 'ab.m.d']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_dom_ex[\"event_date\"], df_dom_ex[\"count_views\"], marker='o')\n",
    "plt.title(\"Daily Active Views Trend for ab.m.d\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Views\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3f1a63a5-2d14-43d5-9068-21a676205b07",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_dom = df_fe.copy()\n",
    "\n",
    "df_dom[\"event_date\"] = pd.to_datetime(df_dom[\"event_date\"])\n",
    "df_dom_ex = df_dom.loc[df_dom[\"domain_code\"] == 'ace']\n",
    "\n",
    "plt.figure(figsize=(10,5))\n",
    "plt.plot(df_dom_ex[\"event_date\"], df_dom_ex[\"count_views\"], marker='o')\n",
    "plt.title(\"Daily Active Views Trend for ace\")\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Views\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0246129f-75df-4346-9fc5-efb45c71295f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "20aaaa46-4fe4-4ba5-bac8-e30e0f09dd44",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 1) Churn y Retention Funnel (Overall churn)\n",
    "\n",
    "df_report = spark.createDataFrame(df_fe)\n",
    "window_spec = Window.partitionBy(\"domain_code\")\n",
    "\n",
    "df_report = df_report.select(\n",
    "    F.col(\"domain_code\"),\n",
    "    F.col(\"event_date\"),\n",
    "    F.col(\"count_views\"),\n",
    "    F.col(\"churn\")\n",
    ").withColumn(\"join_date\", F.min(\"event_date\").over(window_spec))\n",
    "\n",
    "df_report.write.format(\"delta\").mode(\"overwrite\").saveAsTable(\"projectviews.gold.vw_churn_retention\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2edc3927-42ee-429e-9b00-d18ece57796b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_report.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03731629-9c80-4baf-9c4f-16cbd37cb5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd57703-fa62-4198-9828-2424803395d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "def preprocess_categorical_and_datetime(df, cat_columns=None, datetime_columns=None):\n",
    "    \"\"\"\n",
    "    Preprocess categorical and datetime columns.\n",
    "    \n",
    "    - Label encode categorical columns\n",
    "    - Convert datetime columns to datetime type and extract features\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original dataframe\n",
    "        cat_columns (list): List of categorical columns to encode (optional)\n",
    "        datetime_columns (list): List of datetime columns to process (optional)\n",
    "    \n",
    "    Returns:\n",
    "        df_processed (pd.DataFrame): DataFrame with processed features\n",
    "        encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "        domain_mapping (dict): Mapping from encoded values back to original domain_code values\n",
    "    \"\"\"\n",
    "    df_processed = df.copy()\n",
    "    encoders = {}\n",
    "    domain_mapping = {}  # Store domain_code mapping\n",
    "    \n",
    "    # Remove nulls\n",
    "    cols_to_check = ['views_plus_2', 'views_plus_3', 'views_plus_4', 'views_plus_5', 'views_plus_6', 'views_plus_7']\n",
    "    df_processed = df_processed.dropna(subset=cols_to_check)  # Fixed: use df_processed\n",
    "    \n",
    "    # Detect categorical columns automatically if not provided\n",
    "    if cat_columns is None:\n",
    "        cat_columns = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "        # Remove datetime columns from categorical columns if datetime_columns given\n",
    "        if datetime_columns is not None:\n",
    "            cat_columns = [col for col in cat_columns if col not in datetime_columns]\n",
    "    \n",
    "    # Process datetime columns\n",
    "    if datetime_columns is None:\n",
    "        datetime_columns = []\n",
    "    \n",
    "    for col in datetime_columns:\n",
    "        df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
    "        df_processed[f'{col}_year'] = df_processed[col].dt.year\n",
    "        df_processed[f'{col}_month'] = df_processed[col].dt.month\n",
    "        df_processed[f'{col}_day'] = df_processed[col].dt.day\n",
    "        df_processed[f'{col}_dayofweek'] = df_processed[col].dt.dayofweek\n",
    "        df_processed[f'{col}_quarter'] = df_processed[col].dt.quarter\n",
    "        df_processed[f'{col}_is_weekend'] = (df_processed[col].dt.dayofweek >= 5).astype(int)\n",
    "        df_processed = df_processed.drop(columns=[col])\n",
    "    \n",
    "    # Process categorical columns with Label Encoding\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        encoders[col] = le\n",
    "        \n",
    "        # If it's domain_code, create inverse mapping\n",
    "        if col == 'domain_code':\n",
    "            domain_mapping = {\n",
    "                encoded: original for encoded, original \n",
    "                in zip(le.transform(le.classes_), le.classes_)\n",
    "            }\n",
    "    \n",
    "    return df_processed, encoders, domain_mapping\n",
    "\n",
    "def cross_validate_lgb_explain(X, y, params, domain_mapping=None, n_splits=5):\n",
    "    \"\"\"\n",
    "    Cross-validate LightGBM model with SHAP explanations.\n",
    "    \n",
    "    Parameters:\n",
    "        X (pd.DataFrame): Features\n",
    "        y (pd.Series): Target variable\n",
    "        params (dict): LightGBM parameters\n",
    "        domain_mapping (dict): Mapping from encoded domain_code to original values\n",
    "        n_splits (int): Number of CV folds\n",
    "    \n",
    "    Returns:\n",
    "        results_df (pd.DataFrame): DataFrame with predictions, true labels, and SHAP values\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    \n",
    "    roc_auc_scores = []\n",
    "    pr_auc_scores = []\n",
    "    all_preds = pd.Series(index=X.index, dtype=float)  # store risk scores\n",
    "    \n",
    "    # store shap values from all folds\n",
    "    shap_values_all = np.zeros(X.shape)\n",
    "    \n",
    "    for fold, (train_idx, valid_idx) in enumerate(cv.split(X, y), 1):\n",
    "        X_train, X_valid = X.iloc[train_idx], X.iloc[valid_idx]\n",
    "        y_train, y_valid = y.iloc[train_idx], y.iloc[valid_idx]\n",
    "        \n",
    "        train_data = lgb.Dataset(X_train, label=y_train)\n",
    "        valid_data = lgb.Dataset(X_valid, label=y_valid, reference=train_data)\n",
    "        \n",
    "        model = lgb.train(\n",
    "            params,\n",
    "            train_data,\n",
    "            valid_sets=[valid_data],\n",
    "            callbacks=[lgb.log_evaluation(10)]\n",
    "        )\n",
    "        \n",
    "        # Risk score predictions\n",
    "        y_pred = model.predict(X_valid)\n",
    "        all_preds.iloc[valid_idx] = y_pred\n",
    "        \n",
    "        # Metrics\n",
    "        roc_auc = roc_auc_score(y_valid, y_pred)\n",
    "        pr_auc = average_precision_score(y_valid, y_pred)\n",
    "        roc_auc_scores.append(roc_auc)\n",
    "        pr_auc_scores.append(pr_auc)\n",
    "        \n",
    "        # SHAP by fold\n",
    "        explainer = shap.TreeExplainer(model)\n",
    "        shap_values_fold = explainer.shap_values(X_valid)\n",
    "        shap_values_all[valid_idx, :] = shap_values_fold\n",
    "    \n",
    "    print(f\"Mean ROC-AUC: {np.mean(roc_auc_scores):.4f} ± {np.std(roc_auc_scores):.4f}\")\n",
    "    print(f\"Mean PR-AUC: {np.mean(pr_auc_scores):.4f} ± {np.std(pr_auc_scores):.4f}\")\n",
    "    \n",
    "    # Create results DataFrame\n",
    "    shap_df = pd.DataFrame(shap_values_all, columns=X.columns, index=X.index)\n",
    "    results_df = X.copy()\n",
    "    results_df['risk_score'] = all_preds\n",
    "    results_df['true_label'] = y\n",
    "    results_df = pd.concat([results_df, shap_df.add_prefix(\"shap_\")], axis=1)\n",
    "    \n",
    "    # Recover original domain_code values if mapping exists\n",
    "    if domain_mapping is not None and 'domain_code' in results_df.columns:\n",
    "        results_df['domain_code_original'] = results_df['domain_code'].map(domain_mapping)\n",
    "        print(f\"Original domain_code values recovered\")\n",
    "        print(f\"Mapping example: {dict(list(domain_mapping.items())[:5])}\")\n",
    "    \n",
    "    # Global plots\n",
    "    shap.summary_plot(shap_values_all, X, plot_type=\"bar\", show=False)\n",
    "    shap.summary_plot(shap_values_all, X, show=False)\n",
    "    \n",
    "    return results_df\n",
    "\n",
    "# Complete usage example:\n",
    "def run_complete_pipeline(df_fe):\n",
    "    \"\"\"\n",
    "    Complete pipeline with domain_code original value recovery\n",
    "    \"\"\"\n",
    "    print(\"Starting complete pipeline...\")\n",
    "    \n",
    "    # 1. Preprocessing\n",
    "    df_processed, encoders, domain_mapping = preprocess_categorical_and_datetime(\n",
    "        df_fe, \n",
    "        datetime_columns=['event_date']  # Specify datetime columns\n",
    "    )\n",
    "    \n",
    "    print(f\"Preprocessing completed\")\n",
    "    print(f\"Shape after preprocessing: {df_processed.shape}\")\n",
    "    \n",
    "    # 2. Prepare data for model\n",
    "    X = df_processed.drop(columns=['churn'])\n",
    "    y = df_processed['churn']\n",
    "    \n",
    "    # 3. Model parameters (you can use Optuna-optimized ones)\n",
    "    params = {\n",
    "        'objective': 'binary',\n",
    "        'metric': 'auc',\n",
    "        'verbosity': -1,\n",
    "        'boosting_type': 'gbdt',\n",
    "        'learning_rate': 0.1,\n",
    "        'num_leaves': 100,\n",
    "        'max_depth': 6,\n",
    "        'min_data_in_leaf': 50,\n",
    "        'feature_fraction': 0.8,\n",
    "        'bagging_fraction': 0.8,\n",
    "        'bagging_freq': 5,\n",
    "        'random_state': 42,\n",
    "        'force_col_wise': True\n",
    "    }\n",
    "    \n",
    "    # 4. Cross-validation with SHAP and domain_code recovery\n",
    "    results_df = cross_validate_lgb_explain(X, y, params, domain_mapping)\n",
    "    \n",
    "    print(f\"\\n Final results:\")\n",
    "    print(f\"Columns in results: {results_df.columns.tolist()}\")\n",
    "    \n",
    "    # Show examples with original domain_code\n",
    "    if 'domain_code_original' in results_df.columns:\n",
    "        print(f\"\\n Sample results with original domain_code:\")\n",
    "        sample_cols = ['domain_code', 'domain_code_original', 'risk_score', 'true_label']\n",
    "        available_cols = [col for col in sample_cols if col in results_df.columns]\n",
    "        print(results_df[available_cols].head(10))\n",
    "    \n",
    "    return results_df, encoders, domain_mapping\n",
    "\n",
    "# Call complete function (uncomment to use):\n",
    "results_df, encoders, domain_mapping = run_complete_pipeline(df_fe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a14a4091-163d-4573-896f-adeff7f609c6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "24c49213-8b86-46e8-9440-ac38a8eea004",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37d1effa-b2ca-4ccc-b199-c14028893ef1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "### 3) model outputs\n",
    "\n",
    "vw_model_prediction = spark.createDataFrame(results_df)\n",
    "THR = 0.5\n",
    "vw_model_prediction = (\n",
    "    vw_model_prediction\n",
    "    .withColumn(\n",
    "        \"event_date\",\n",
    "        F.make_date(\n",
    "            F.col(\"event_date_year\").cast(\"int\"),\n",
    "            F.col(\"event_date_month\").cast(\"int\"),\n",
    "            F.col(\"event_date_day\").cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"risk_score\", F.col(\"risk_score\").cast(\"double\"))\n",
    "    .withColumn(\"predicted_churn\", (F.col(\"risk_score\") >= F.lit(THR)).cast(\"int\"))\n",
    "    # .select(\n",
    "    #     F.col(\"domain_code_original\").alias(\"domain_code\"),\n",
    "    #     \"event_date\",\n",
    "    #     \"count_views\",\n",
    "    #     F.col(\"true_label\").alias(\"churn\"),\n",
    "    #     \"risk_score\",\n",
    "    #     \"predicted_churn\"\n",
    "    # )\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0aa1828f-aea8-4ce5-9321-4b3467db36c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "(vw_model_prediction\n",
    ".write\n",
    ".format(\"delta\")\n",
    ".option(\"mergeSchema\", True)\n",
    ".mode(\"overwrite\")\n",
    ".saveAsTable(\"projectviews.gold.vw_model_prediction\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0010118a-2325-4a2f-9689-c63ba1a7712e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "\n",
    "select * from projectviews.gold.vw_model_prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8a069536-51a7-437e-af7f-447a59a55b32",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "500443ed-d3e6-46b0-b8a3-59ade85bab7c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "THR = 0.5  \n",
    "\n",
    "y_true = results_df['true_label'].astype(int).values\n",
    "y_prob = results_df['risk_score'].astype(float).values\n",
    "mask = (y_prob >= 0.0) & (y_prob <= 1.0)\n",
    "y_true = y_true[mask]\n",
    "y_prob = y_prob[mask]\n",
    "y_pred = (y_prob >= THR).astype(int)\n",
    "\n",
    "\n",
    "plt.figure()\n",
    "plt.hist(y_prob, bins=np.linspace(0, 1, 51))\n",
    "plt.axvline(THR, linestyle='--')\n",
    "plt.xlabel('risk_score')\n",
    "plt.ylabel('count')\n",
    "plt.title('Distribución de risk_score')\n",
    "plt.show()\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]).plot(values_format='d')\n",
    "plt.title(f'Matriz de confusión (thr={THR})')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('FPR')\n",
    "plt.ylabel('TPR')\n",
    "plt.title('ROC curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "\n",
    "prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "ap = average_precision_score(y_true, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f'AP = {ap:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision–Recall curve')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5213229790752764,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Feature_Engineering",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

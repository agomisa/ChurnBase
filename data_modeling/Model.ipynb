{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "69eaa54b-55f0-41f9-9f4d-233d72cc251c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt\n",
    "dbutils.library.restartPython()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b127ef4f-f74c-4120-a02b-b7a796e9a2b0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import lightgbm as lgb\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import StratifiedKFold, ParameterGrid\n",
    "import shap\n",
    "import optuna\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay,roc_auc_score, roc_curve, auc, precision_recall_curve, average_precision_score,classification_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5295bacc-0106-4de9-b257-42d8c405b6af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Read"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "92e9ed71-1bea-4767-ba72-cb601b4ab502",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.table(\"projectviews.default.gold_projectviews_fe\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "25ac2ff7-356d-412e-920d-a1253434d7a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "41af895a-b24f-49ce-9bc6-b4562f768568",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def preprocess_categorical_and_datetime(df, cat_columns=None, datetime_columns=None):\n",
    "    \"\"\"\n",
    "    - Preprocess categorical and datetime columns.\n",
    "    - Drop columns refers to the future.\n",
    "    - Label encode categorical columns.\n",
    "    - Convert datetime columns to datetime type and extract features.\n",
    "    \n",
    "    Parameters:\n",
    "        df (pd.DataFrame): Original dataframe\n",
    "        cat_columns (list): List of categorical columns to encode (optional)\n",
    "        datetime_columns (list): List of datetime columns to process (optional)\n",
    "    \n",
    "    Returns:\n",
    "        df_processed (pd.DataFrame): DataFrame with processed features\n",
    "        encoders (dict): Dictionary of LabelEncoders for categorical columns\n",
    "        domain_mapping (dict): Mapping from encoded values back to original domain_code values\n",
    "    \"\"\"\n",
    "    df_processed = df.toPandas()\n",
    "    encoders = {}\n",
    "    domain_mapping = {}  \n",
    "    \n",
    "    # Remove future columns\n",
    "    cols_to_drop = [\n",
    "        'views_plus_1',\n",
    "        'views_plus_2',\n",
    "        'views_plus_3',\n",
    "        'views_plus_4',\n",
    "        'views_plus_5',\n",
    "        'views_plus_6',\n",
    "        'views_plus_7',\n",
    "        'min_views_future'\n",
    "    ]\n",
    "    df_processed = df_processed.drop(columns=cols_to_drop)\n",
    "    \n",
    "    # Detect categorical columns automatically if not provided\n",
    "    if cat_columns is None:\n",
    "        cat_columns = df_processed.select_dtypes(include=['object']).columns.tolist()\n",
    "        # Remove datetime columns from categorical columns if datetime_columns given\n",
    "        if datetime_columns is not None:\n",
    "            cat_columns = [col for col in cat_columns if col not in datetime_columns]\n",
    "    \n",
    "    # Process datetime columns\n",
    "    if datetime_columns is None:\n",
    "        datetime_columns = []\n",
    "    \n",
    "    for col in datetime_columns:\n",
    "        df_processed[col] = pd.to_datetime(df_processed[col], errors='coerce')\n",
    "        df_processed[f'{col}_year'] = df_processed[col].dt.year\n",
    "        df_processed[f'{col}_month'] = df_processed[col].dt.month\n",
    "        df_processed[f'{col}_day'] = df_processed[col].dt.day\n",
    "        df_processed[f'{col}_dayofweek'] = df_processed[col].dt.dayofweek\n",
    "        df_processed[f'{col}_quarter'] = df_processed[col].dt.quarter\n",
    "        df_processed[f'{col}_is_weekend'] = (df_processed[col].dt.dayofweek >= 5).astype(int)\n",
    "        df_processed = df_processed.drop(columns=[col])\n",
    "    \n",
    "    # Process categorical columns with Label Encoding\n",
    "    for col in cat_columns:\n",
    "        le = LabelEncoder()\n",
    "        df_processed[col] = le.fit_transform(df_processed[col].astype(str))\n",
    "        encoders[col] = le\n",
    "        \n",
    "        # If it's domain_code, create inverse mapping\n",
    "        if col == 'domain_code':\n",
    "            domain_mapping = {\n",
    "                encoded: original for encoded, original \n",
    "                in zip(le.transform(le.classes_), le.classes_)\n",
    "            }\n",
    "    \n",
    "    return df_processed, encoders, domain_mapping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c623cede-f386-4abb-a9af-b50e5f2edafa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_featured, encoders, domain_mapping = preprocess_categorical_and_datetime(df, cat_columns=['domain_code'], datetime_columns=['event_date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5fa3e0a-4063-41fa-a793-d4deeb2023ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "y = df_featured['churn'].astype(int)\n",
    "# Compute class ratio from your labels\n",
    "pos = int((y == 1).sum())\n",
    "neg = int((y == 0).sum())\n",
    "scale_pos_weight = (neg / max(pos, 1))\n",
    "print(f\"Class ratio -> pos={pos}, neg={neg}, scale_pos_weight={scale_pos_weight:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "03731629-9c80-4baf-9c4f-16cbd37cb5c3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bc0dd652-2f57-40f4-9776-10ea4744d775",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Without Balancing"
    }
   },
   "outputs": [],
   "source": [
    "def run_grid_search_without_balance(X, y, param_grid, n_splits=5):\n",
    "    \"\"\"\n",
    "    Perform a manual grid search with Stratified K-Fold cross-validation\n",
    "    to find the best LightGBM hyperparameters based on AUC score.\n",
    "\n",
    "    Parameters:\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix.\n",
    "    y : pandas.Series or array-like\n",
    "        Target variable (binary classification).\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameters to test, where each key is a parameter\n",
    "        and each value is a list of possible values to try.\n",
    "    n_splits : int, default=5\n",
    "        Number of folds for Stratified K-Fold cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    best_params_full : dict\n",
    "        Dictionary of the best parameters found (including fixed parameters).\n",
    "    \"\"\"\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_score = -np.inf\n",
    "    best_params_full = None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        params_full = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'force_col_wise': True,\n",
    "            **params\n",
    "        }\n",
    "        fold_scores = []\n",
    "        for tr_idx, va_idx in cv.split(X, y):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            dtr = lgb.Dataset(X_tr, label=y_tr)\n",
    "            dva = lgb.Dataset(X_va, label=y_va, reference=dtr)\n",
    "\n",
    "            model = lgb.train(params_full, dtr, valid_sets=[dva])\n",
    "            preds = model.predict(X_va)\n",
    "            fold_scores.append(roc_auc_score(y_va, preds))\n",
    "\n",
    "        mean_auc = float(np.mean(fold_scores))\n",
    "        print(f\"Params {params} | AUC={mean_auc:.4f}\")\n",
    "        if mean_auc > best_score:\n",
    "            best_score = mean_auc\n",
    "            best_params_full = params_full\n",
    "\n",
    "    print(\"Best Parameters:\", best_params_full)\n",
    "    print(\"Best AUC-CV:\", best_score)\n",
    "    return best_params_full"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b034656d-6573-4f96-b8c2-a5d30bca87c8",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Balance Class"
    }
   },
   "outputs": [],
   "source": [
    "X = df_featured.drop(columns=['churn'])\n",
    "y = df_featured['churn'].astype(int)\n",
    "\n",
    "param_grid = {\n",
    "    'learning_rate': [0.05, 0.1],\n",
    "    'num_leaves': [31, 63],\n",
    "    'max_depth': [4, 6],\n",
    "    'min_data_in_leaf': [20],\n",
    "    'feature_fraction': [0.8],\n",
    "    'bagging_fraction': [0.8],\n",
    "    'bagging_freq': [1]\n",
    "}\n",
    "\n",
    "def run_grid_search(X, y, param_grid, n_splits=5, use_scale_pos_weight=True):\n",
    "    \"\"\"\n",
    "    Perform a manual grid search with Stratified K-Fold cross-validation\n",
    "    to find the best LightGBM hyperparameters based on AUC score.\n",
    "\n",
    "    Parameters:\n",
    "    X : pandas.DataFrame\n",
    "        Feature matrix.\n",
    "    y : pandas.Series or array-like\n",
    "        Target variable (binary classification).\n",
    "    param_grid : dict\n",
    "        Dictionary of hyperparameters to test, where each key is a parameter\n",
    "        and each value is a list of possible values to try.\n",
    "    n_splits : int, default=5\n",
    "        Number of folds for Stratified K-Fold cross-validation.\n",
    "\n",
    "    Returns:\n",
    "    best_params_full : dict\n",
    "        Dictionary of the best parameters found (including fixed parameters).\n",
    "    \"\"\"\n",
    "\n",
    "    cv = StratifiedKFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    best_score = -np.inf\n",
    "    best_params_full = None\n",
    "\n",
    "    pos = int((y == 1).sum())\n",
    "    neg = int((y == 0).sum())\n",
    "    spw = (neg / max(pos, 1)) if use_scale_pos_weight else None\n",
    "\n",
    "    for params in ParameterGrid(param_grid):\n",
    "        params_full = {\n",
    "            'objective': 'binary',\n",
    "            'metric': 'auc',            # we'll also compute AP manually\n",
    "            'verbosity': -1,\n",
    "            'boosting_type': 'gbdt',\n",
    "            'random_state': 42,\n",
    "            'force_col_wise': True,\n",
    "            **params\n",
    "        }\n",
    "        if use_scale_pos_weight:\n",
    "            params_full['scale_pos_weight'] = spw\n",
    "        else:\n",
    "            params_full['is_unbalance'] = True\n",
    "\n",
    "        fold_auc, fold_ap = [], []\n",
    "\n",
    "        for tr_idx, va_idx in cv.split(X, y):\n",
    "            X_tr, X_va = X.iloc[tr_idx], X.iloc[va_idx]\n",
    "            y_tr, y_va = y.iloc[tr_idx], y.iloc[va_idx]\n",
    "\n",
    "            dtr = lgb.Dataset(X_tr, label=y_tr)\n",
    "            dva = lgb.Dataset(X_va, label=y_va, reference=dtr)\n",
    "\n",
    "            model = lgb.train(\n",
    "                params_full,\n",
    "                dtr,\n",
    "                valid_sets=[dva],\n",
    "                callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    "            )\n",
    "            p = model.predict(X_va, num_iteration=model.best_iteration)\n",
    "            fold_auc.append(roc_auc_score(y_va, p))\n",
    "            fold_ap.append(average_precision_score(y_va, p))\n",
    "\n",
    "        mean_auc, mean_ap = float(np.mean(fold_auc)), float(np.mean(fold_ap))\n",
    "        print(f\"Params {params} | AUC={mean_auc:.4f} | AP={mean_ap:.4f}\")\n",
    "\n",
    "        # Prefer AP when imbalanced; fall back to AUC if you want\n",
    "        score_for_selection = mean_ap\n",
    "        if score_for_selection > best_score:\n",
    "            best_score = score_for_selection\n",
    "            best_params_full = params_full\n",
    "\n",
    "    print(\"Best Parameters:\", best_params_full)\n",
    "    print(\"Best CV (AP):\", best_score)\n",
    "    return best_params_full\n",
    "\n",
    "best_params = run_grid_search(X, y, param_grid, n_splits=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "af6f1534-822e-400a-a23b-d418b8547602",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Final Model"
    }
   },
   "outputs": [],
   "source": [
    "## Final Model\n",
    "final_train = lgb.Dataset(X, label=y)\n",
    "final_model = lgb.train(\n",
    "    {**best_params, 'objective': 'binary', 'metric': 'auc', 'verbosity': -1, 'random_state': 42},\n",
    "    final_train\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a04e7ec1-ea71-411a-9820-ed675706cfe7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b311b14-cd13-45c6-8f4c-ba913dfd7520",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Prediction"
    }
   },
   "outputs": [],
   "source": [
    "# Predicts\n",
    "preds = final_model.predict(X)\n",
    "df_results = df_featured.copy()\n",
    "df_results['risk_score'] = preds\n",
    "\n",
    "display(df_results.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "75491f08-4d80-416a-a101-d3124fbebb7b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Explainability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d73de8ed-70ea-44bf-b1e3-6841f24e7519",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "Shap Analysis"
    }
   },
   "outputs": [],
   "source": [
    "## Shap\n",
    "explainer = shap.TreeExplainer(final_model)\n",
    "shap_values = explainer.shap_values(X)\n",
    "\n",
    "shap.summary_plot(shap_values, X, plot_type=\"bar\")\n",
    "shap.summary_plot(shap_values, X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c5b47545-f43d-49f4-87d2-60eef7d7c792",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Model Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6fe0d33a-217d-4f69-a4e1-af41831d56af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# 1) Prepare data\n",
    "THR = 0.5  # default threshold\n",
    "y_true = df_results['churn'].astype(int).values # ground truth (0/1)\n",
    "y_prob = df_results['risk_score'].astype(float).values # predicted probabilities in [0, 1]\n",
    "\n",
    "# Keep only valid probabilities\n",
    "mask = (y_prob >= 0.0) & (y_prob <= 1.0)\n",
    "y_true = y_true[mask]\n",
    "y_prob = y_prob[mask]\n",
    "\n",
    "# 2) Choose a better threshold (maximize F1)\n",
    "prec, rec, thr_grid = precision_recall_curve(y_true, y_prob)\n",
    "f1_scores = (2 * prec * rec) / (prec + rec + 1e-12)\n",
    "best_idx = np.nanargmax(f1_scores)\n",
    "best_thr = thr_grid[max(best_idx - 1, 0)] if len(thr_grid) > 0 else THR\n",
    "print(f\"Suggested threshold by max-F1: {best_thr:.3f}\")\n",
    "# Use either the suggested threshold or stick to 0.5\n",
    "THR = float(best_thr)  # or keep THR = 0.5\n",
    "\n",
    "# 3) Hard predictions\n",
    "y_pred = (y_prob >= THR).astype(int)\n",
    "\n",
    "# 4) Score distribution\n",
    "plt.figure()\n",
    "plt.hist(y_prob, bins=np.linspace(0, 1, 51))\n",
    "plt.axvline(THR, linestyle='--')\n",
    "plt.xlabel('risk_score')\n",
    "plt.ylabel('count')\n",
    "plt.title('Risk score distribution')\n",
    "plt.show()\n",
    "\n",
    "# 5) Confusion Matrix\n",
    "cm = confusion_matrix(y_true, y_pred, labels=[0, 1])\n",
    "ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[0, 1]).plot(values_format='d')\n",
    "plt.title(f'Confusion Matrix (thr={THR:.3f})')\n",
    "plt.show()\n",
    "\n",
    "# 6) ROC curve & AUC\n",
    "fpr, tpr, _ = roc_curve(y_true, y_prob)\n",
    "roc_auc = auc(fpr, tpr)\n",
    "plt.figure()\n",
    "plt.plot(fpr, tpr, label=f'AUC = {roc_auc:.3f}')\n",
    "plt.plot([0, 1], [0, 1], linestyle='--')\n",
    "plt.xlabel('False Positive Rate')\n",
    "plt.ylabel('True Positive Rate')\n",
    "plt.title('ROC curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 7) Precision–Recall curve & Average Precision\n",
    "prec, rec, _ = precision_recall_curve(y_true, y_prob)\n",
    "ap = average_precision_score(y_true, y_prob)\n",
    "plt.figure()\n",
    "plt.plot(rec, prec, label=f'AP = {ap:.3f}')\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.title('Precision–Recall curve')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 8) Detailed metrics\n",
    "print(classification_report(y_true, y_pred, digits=4))\n",
    "print(f\"AUC: {roc_auc:.4f} | AP: {ap:.4f} | Threshold used: {THR:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "df43f79c-00af-4def-94a1-36d9438598bf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Output Table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3a974023-61da-4b6f-bce1-96f0c2d7630a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Threshold for classification\n",
    "THR = 0.5\n",
    "\n",
    "# If mapping is available, add original domain_code\n",
    "if domain_mapping is not None and 'domain_code' in df_results.columns:\n",
    "    df_results['domain_code_original'] = df_results['domain_code'].map(domain_mapping)\n",
    "    print(\"Original domain_code values recovered\")\n",
    "    print(f\"Mapping example: {dict(list(domain_mapping.items())[:5])}\")\n",
    "\n",
    "# Convert to Spark DataFrame again if mapping was added\n",
    "vw_model_prediction = spark.createDataFrame(df_results)\n",
    "\n",
    "# Build event_date and predicted churn\n",
    "vw_model_prediction = (\n",
    "    vw_model_prediction\n",
    "    .withColumn(\n",
    "        \"event_date\",\n",
    "        F.make_date(\n",
    "            F.col(\"event_date_year\").cast(\"int\"),\n",
    "            F.col(\"event_date_month\").cast(\"int\"),\n",
    "            F.col(\"event_date_day\").cast(\"int\")\n",
    "        )\n",
    "    )\n",
    "    .withColumn(\"risk_score\", F.col(\"risk_score\").cast(\"double\"))\n",
    "    .withColumn(\"predicted_churn\", (F.col(\"risk_score\") >= F.lit(THR)).cast(\"int\"))\n",
    "    .drop(\"event_date_year\", \"event_date_month\", \"event_date_day\", \"event_date_dayofweek\", \"event_date_quarter\", \"event_date_is_weekend\")\n",
    ")\n",
    "\n",
    "# Save as Delta table\n",
    "(vw_model_prediction\n",
    "    .write\n",
    "    .format(\"delta\")\n",
    "    .option(\"mergeSchema\", True)\n",
    "    .mode(\"overwrite\")\n",
    "    .saveAsTable(\"projectviews.default.vw_model_prediction\")\n",
    ")\n",
    "\n",
    "print(\"vw_model_prediction table successfully created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ff3a4b12-054b-41d7-9b7c-e27518041ae5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "vw_model_prediction.columns"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5213229790752764,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "Model",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}

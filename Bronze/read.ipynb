{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e9fa159b-8a33-4eb7-a59a-1b6afd179d74",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import gzip\n",
    "import os\n",
    "import re\n",
    "from datetime import datetime\n",
    "import pyspark.sql.functions as F\n",
    "from dotenv import load_dotenv\n",
    "from pyspark.sql import SparkSession,DataFrame\n",
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType\n",
    "from functools import reduce\n",
    "from collections import defaultdict\n",
    "\n",
    "load_dotenv()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "290f5386-047c-42ca-96c9-f50d9211ebee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df = spark.read.option(\"header\", \"true\").csv(\"s3://dest-wikimedia/pageviews/2025/2025-01/*.gz\")\n",
    "df.write.mode(\"overwrite\").parquet(\"/tmp/wikimedia/pageviews/2025/2025-01/\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "07bc7b83-ff86-414a-a597-1d74ea8566d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "display(dbutils.fs.mounts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a037dfe-842f-4c7b-8af6-917f10ee059a",
     "showTitle": true,
     "tableResultSettingsMap": {},
     "title": "VARIABLES"
    }
   },
   "outputs": [],
   "source": [
    "bucket = 'dest-wikimedia'\n",
    "table_name = \"workspace.default.wikimedia_pageviews\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "73e2e400-b8f3-4c16-9b01-e90e6d58bfc2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "aws_access_key_id = os.getenv('AWS_ACCESS_KEY_ID')\n",
    "aws_secret_access_key = os.getenv('AWS_SECRET_ACCESS_KEY')\n",
    "aws_region = os.getenv('AWS_REGION', 'eu-central-1')\n",
    "\n",
    "def get_s3_client(aws_access_key_id, aws_secret_access_key, region_name='eu-central-1'):\n",
    "    \"\"\"Return a boto3 S3 client given the access and secret keys.\"\"\"\n",
    "    s3_client = boto3.client(\n",
    "        's3',\n",
    "        aws_access_key_id=aws_access_key_id,\n",
    "        aws_secret_access_key=aws_secret_access_key,\n",
    "        region_name=region_name\n",
    "    )\n",
    "    return s3_client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "295de34e-8c4a-44ce-9e60-1fe42845f999",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "s3_client = get_s3_client(aws_access_key_id, aws_secret_access_key)\n",
    "response = s3_client.list_objects_v2(Bucket=bucket)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "42854e73-3cfd-4725-9ed0-4d0e047ccf4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import datetime\n",
    "import calendar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c8d8c41e-d963-41f6-ab92-5321dc167527",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def generate_date_range(start_date, end_date):\n",
    "    current = start_date\n",
    "    while current <= end_date:\n",
    "        yield current.strftime('%Y-%m-%d')\n",
    "        current += datetime.timedelta(days=1)\n",
    "\n",
    "# Parámetros de entrada\n",
    "s3_base_path = \"s3://dest-wikimedia/pageviews\"\n",
    "start_date = datetime.date(2025, 1, 1)\n",
    "end_date = datetime.date(2025, 1, 31)  # Cambiá si querés otro rango\n",
    "\n",
    "for day_str in generate_date_range(start_date, end_date):\n",
    "    print(f\"Procesando: {day_str}\")\n",
    "    year, month, day = day_str.split(\"-\")\n",
    "    \n",
    "    try:\n",
    "        path = f\"{s3_base_path}/{year}/{year}-{month}/pageviews-{year}{month}{day}-*.gz\"\n",
    "        raw_df = spark.read.text(path)\n",
    "        \n",
    "        if raw_df.isEmpty():\n",
    "            print(f\"No hay archivos para {day_str}\")\n",
    "            continue\n",
    "        \n",
    "        # Procesamiento\n",
    "        split_col = F.split(raw_df[\"value\"], \" \")\n",
    "\n",
    "        parsed_df = raw_df.withColumn(\"domain_code\", split_col.getItem(0))\\\n",
    "            .withColumn(\"page_title\", split_col.getItem(1))\\\n",
    "            .withColumn(\"count_views\", split_col.getItem(2).cast(\"int\"))\\\n",
    "            .withColumn(\"total_response_size\", split_col.getItem(3).cast(\"int\"))\\\n",
    "            .withColumn(\"file_path\", F.col(\"_metadata.file_path\"))\\\n",
    "            .withColumn(\"timestamp_str\", F.regexp_extract(F.col(\"file_path\"), r'pageviews-(\\d{8}-\\d{6})\\.gz', 1))\\\n",
    "            .withColumn(\"event_timestamp\", F.to_timestamp(\"timestamp_str\", \"yyyyMMdd-HHmmss\"))\\\n",
    "            .withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\\\n",
    "            .drop(\"value\", \"file_path\", \"timestamp_str\")\n",
    "        \n",
    "        parsed_df.write.format(\"delta\")\\\n",
    "            .mode(\"append\")\\\n",
    "            .partitionBy(\"event_date\")\\\n",
    "            .saveAsTable(\"workspace.default.wikimedia_pageviews\")\n",
    "\n",
    "        print(f\"✔ Día {day_str} procesado\")\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"❌ Error procesando {day_str}: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "96bce75d-369a-4d8c-b24a-635c85c92fff",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Contained data\n",
    "\n",
    "https://wikitech.wikimedia.org/wiki/Data_Platform/Data_Lake/Traffic/Pageviews\n",
    "\n",
    "domain_code | page_title| count_views| total_response_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a221a80d-ba0d-4b01-96d5-5368c9087e52",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def read_gz_lines_from_s3(s3_client, bucket, key):\n",
    "    obj = s3_client.get_object(Bucket=bucket, Key=key)\n",
    "    with gzip.GzipFile(fileobj=io.BytesIO(obj[\"Body\"].read()), mode=\"rb\") as f:\n",
    "        for line in f:\n",
    "            yield line.decode(\"utf-8\").strip()\n",
    "\n",
    "def extract_timestamp_from_key(key):\n",
    "    # path ejemplo: 'pageviews-20250101-000000.gz'\n",
    "    match = re.search(r'pageviews-(\\d{8})-(\\d{6})\\.gz', key)\n",
    "    if match:\n",
    "        date_str = match.group(1)  # '20250101'\n",
    "        time_str = match.group(2)  # '000000'\n",
    "        timestamp_str = date_str + time_str  # '20250101000000'\n",
    "        return datetime.strptime(timestamp_str, '%Y%m%d%H%M%S')\n",
    "    else:\n",
    "        return None\n",
    "\n",
    "def parse_line(line):\n",
    "    if line.startswith('\"\" '):\n",
    "        return None\n",
    "\n",
    "    parts = line.split(\" \", 3)\n",
    "    if len(parts) == 4:\n",
    "        project, page_title, view_count, response_size = parts\n",
    "        return (\n",
    "            project.replace('\"', ''),\n",
    "            page_title,\n",
    "            int(view_count),\n",
    "            int(response_size)\n",
    "        )\n",
    "    else:\n",
    "        return None\n",
    "    \n",
    "schema = StructType([\n",
    "    StructField(\"domain_code\", StringType(), True),\n",
    "    StructField(\"page_title\", StringType(), True),\n",
    "    StructField(\"count_views\", IntegerType(), True),\n",
    "    StructField(\"total_response_size\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "def process_file(s3_client, bucket, key):\n",
    "    timestamp = extract_timestamp_from_key(key)\n",
    "    rows = []\n",
    "    for line in read_gz_lines_from_s3(s3_client, bucket, key):\n",
    "        if line.startswith('\"\"'):  \n",
    "            continue\n",
    "        parsed = parse_line(line)\n",
    "        if parsed:\n",
    "            rows.append(parsed)\n",
    "    df = spark.createDataFrame(rows, schema=schema)\n",
    "    df = df.withColumn(\"event_timestamp\", F.lit(timestamp))\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f4958fc4-0ed9-4ebb-ad31-c971b131cde9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "keys_by_date = defaultdict(list)\n",
    "\n",
    "for key in gz_keys:\n",
    "    ts = extract_timestamp_from_key(key)\n",
    "    if ts:\n",
    "        day_str = ts.strftime(\"%Y-%m-%d\")\n",
    "        keys_by_date[day_str].append(key)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9ef4760d-0bb7-4467-a9ad-01b91411b75f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "file_paths =  [f\"s3a://{bucket}/{obj['Key']}\" for obj in response['Contents'] if obj[\"Key\"].endswith(\".gz\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "30512904-4462-41d0-9b14-2f01d760c59d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "raw_df = spark.read.text(\"s3://dest-wikimedia/pageviews/2025/2025-01/*.gz\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "334d563e-e255-47f7-ac27-2e0db2279b78",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = (\n",
    "    raw_df\n",
    "    .filter(~F.col(\"value\").startswith('\"\"'))\n",
    "    .withColumn(\"split\", F.split(\"value\", \" \", 4))\n",
    "    .filter(F.size(\"split\") == 4) \n",
    "    .select(\n",
    "        F.col(\"split\")[0].alias(\"domain_code\"),\n",
    "        F.col(\"split\")[1].alias(\"page_title\"),\n",
    "        F.col(\"split\")[2].cast(\"int\").alias(\"count_views\"),\n",
    "        F.col(\"split\")[3].cast(\"int\").alias(\"total_response_size\"),\n",
    "        F.col(\"_metadata.file_path\").alias(\"file_path\")\n",
    "    )\n",
    "    .withColumn(\"date_str\", F.regexp_extract(\"file_path\", r\"pageviews-(\\d{8})-(\\d{6})\\.gz\", 1))\n",
    "    .withColumn(\"time_str\", F.regexp_extract(\"file_path\", r\"pageviews-(\\d{8})-(\\d{6})\\.gz\", 2))\n",
    "    .withColumn(\"event_timestamp\", F.to_timestamp(F.concat_ws(\"\", \"date_str\", \"time_str\"), \"yyyyMMddHHmmss\"))\n",
    "    .withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\n",
    "    .drop('date_str')\n",
    "    .drop('time_str')\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c4f19e6-b9d2-483e-848a-af7e1116423b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df.show(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fabc3830-a8a7-486d-930f-1b6ef7bb005a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df = parsed_df.repartition(\"event_date\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8dd28484-765f-4d57-8617-456b7baa500e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "parsed_df.write.format(\"delta\") \\\n",
    "    .option(\"mergeSchema\", \"true\")\\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"event_date\")\\\n",
    "    .saveAsTable(\"workspace.default.wikimedia_pageviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e6220db-f86c-455c-a5d7-5d3239076081",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "matching_keys = [\n",
    "    obj[\"Key\"] for obj in response.get(\"Contents\", [])\n",
    "    if obj[\"Key\"].endswith(\".gz\")\n",
    "]\n",
    "\n",
    "for key in matching_keys:\n",
    "    try:\n",
    "        print(f\"Processing {key}...\")\n",
    "        timestamp = extract_timestamp_from_key(key)\n",
    "        if not timestamp:\n",
    "            print(f\"Could not extract timestamp from {key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        rows = []\n",
    "        for line in read_gz_lines_from_s3(s3_client, bucket, key):\n",
    "            parsed = parse_line(line)\n",
    "            if parsed:\n",
    "                rows.append(parsed)\n",
    "\n",
    "        if not rows:\n",
    "            print(f\"No valid data found in {key}, skipping.\")\n",
    "            continue\n",
    "\n",
    "        df = spark.createDataFrame(rows, schema=schema)\n",
    "        df = df.withColumn(\"event_timestamp\", F.lit(timestamp))\n",
    "        df = df.withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\n",
    "\n",
    "        # Save to Delta, partitioned by event_date\n",
    "        df.coalesce(1).write.format(\"delta\") \\\n",
    "            .mode(\"append\") \\\n",
    "            .partitionBy(\"event_date\") \\\n",
    "            .saveAsTable(table_name)\n",
    "\n",
    "        print(f\"✅ Successfully written: {key}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing {key}: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "54a8daa0-1ad0-49af-a5f1-8ee6187f691c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "for day, keys in keys_by_date.items():\n",
    "    try:\n",
    "        print(f\"Processing day {day} with {len(keys)} files...\")\n",
    "        dfs = []\n",
    "        for key in keys:\n",
    "            print(key)\n",
    "            df = process_file(s3_client, bucket, key)\n",
    "            if df and df.count() > 0:\n",
    "                dfs.append(df)\n",
    "        \n",
    "        if dfs:\n",
    "            full_day_df = reduce(lambda a, b: a.unionByName(b), dfs)\n",
    "            full_day_df = full_day_df.withColumn(\"event_date\", F.to_date(\"event_timestamp\"))\n",
    "            full_day_df.write.format(\"delta\")\\\n",
    "                .mode(\"append\")\\\n",
    "                .partitionBy(\"event_date\")\\\n",
    "                .saveAsTable(\"workspace.default.wikimedia_pageviews\")\n",
    "            print(f\"Day {day} successfully processed\")\n",
    "        else:\n",
    "            print(f\"Day {day} not processed\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error on day {day}: {e}\")"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "2"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "read",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
